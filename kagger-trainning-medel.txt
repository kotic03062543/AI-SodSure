cell1
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

cell2
import os
from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np
from sklearn.model_selection import train_test_split

def load_dataset(base_dir):
    categories = ['fresh', 'rotten']
    images = []
    labels = []

    for idx, category in enumerate(categories):
        category_path = os.path.join(base_dir, category)
        if os.path.exists(category_path):
            for img_name in os.listdir(category_path):
                if img_name.endswith('.jpg'):
                    img_path = os.path.join(category_path, img_name)
                    img = load_img(img_path, target_size=(128,128))
                    img_array = img_to_array(img) / 255.0
                    images.append(img_array)
                    labels.append(idx)
    return np.array(images), np.array(labels)

# ‡πÇ‡∏´‡∏•‡∏î train dataset
train_dir = '/kaggle/input/locbeef-beef-quality-image-dataset/train/train'
X_train, y_train = load_dataset(train_dir)

# ‡πÇ‡∏´‡∏•‡∏î test dataset
test_dir = '/kaggle/input/locbeef-beef-quality-image-dataset/test/test'
X_test, y_test = load_dataset(test_dir)

print(f'Train samples: {len(X_train)}')
print(f'Test samples: {len(X_test)}')


cell3
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

cell4
import matplotlib.pyplot as plt

# ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ accuracy ‡πÅ‡∏•‡∏∞ loss ‡∏à‡∏≤‡∏Å history
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

# Plot Accuracy
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(epochs, acc, 'bo-', label='Training accuracy')
plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot Loss
plt.subplot(1,2,2)
plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()


cell5
from tensorflow.keras.preprocessing import image
import numpy as np

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô path ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)
img_path = '/kaggle/input/locbeef-beef-quality-image-dataset/test/test/rotten/Rotten (2182).jpg'

# ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö input shape ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• (128x128)
img = image.load_img(img_path, target_size=(128, 128))

# ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ‡πÄ‡∏õ‡πá‡∏ô array ‡πÅ‡∏•‡πâ‡∏ß normalize ‡∏Ñ‡πà‡∏≤‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-1
img_array = image.img_to_array(img) / 255.0

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏¥‡∏ï‡∏¥ batch size ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 1 ‡∏ï‡∏±‡∏ß (‡∏£‡∏π‡∏õ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ input ‡πÄ‡∏õ‡πá‡∏ô batch
img_array = np.expand_dims(img_array, axis=0)

# ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÄ‡∏õ‡πá‡∏ô '‡πÄ‡∏ô‡πà‡∏≤' (class 1)
prediction = model.predict(img_array)
pred = float(prediction.squeeze())  # ‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå sigmoid ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0-1

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏î ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏≤ 1 - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡πá‡∏ô '‡πÄ‡∏ô‡πà‡∏≤'
freshness_percent = (1 - pred) * 100

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏ú‡∏• (index 0 = ‡∏™‡∏î, index 1 = ‡πÄ‡∏ô‡πà‡∏≤)
class_names = ['‡∏™‡∏î', '‡πÄ‡∏ô‡πà‡∏≤']

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡πÇ‡∏î‡∏¢‡∏ñ‡πâ‡∏≤ pred < 0.5 ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤ '‡∏™‡∏î'
predicted_class = class_names[int(pred < 0.5)]

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û
print("üì∏ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û...")
print(f"üí° ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏î‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ä‡∏¥‡πâ‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠: {freshness_percent:.2f}%")

# ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ô‡πà‡∏≤ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à
if freshness_percent >= 50:
    print(f"‚úÖ ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ '‡∏™‡∏î' (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à {freshness_percent:.2f}%)")
else:
    print(f"‚ö†Ô∏è ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ '‡πÄ‡∏ô‡πà‡∏≤' (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à {100 - freshness_percent:.2f}%)")
